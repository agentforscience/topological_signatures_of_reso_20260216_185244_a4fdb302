{
  "0411208v1": {
    "title": "A class of tight contact structures on Sigma_2 x I",
    "authors": [
      "Tanya Cofer"
    ],
    "year": 2004,
    "arxiv_id": "0411208v1",
    "pdf_url": "https://arxiv.org/pdf/math/0411208v1",
    "abstract": "We employ cut and paste contact topological techniques to classify some tight contact structures on the closed, oriented genus-2 surface times the interval. A boundary condition is specified so that the Euler class of the of the contact structure vanishes when evaluated on each boundary component. We prove that there exists a unique, non-product tight contact structure in this case."
  },
  "0307092v2": {
    "title": "Extended Bloch group and the Cheeger-Chern-Simons class",
    "authors": [
      "Walter D Neumann"
    ],
    "year": 2003,
    "arxiv_id": "0307092v2",
    "pdf_url": "https://arxiv.org/pdf/math/0307092v2",
    "abstract": "We define an extended Bloch group and show it is naturally isomorphic to H_3(PSL(2,C)^\u03b4;Z). Using the Rogers dilogarithm function this leads to an exact simplicial formula for the universal Cheeger-Chern-Simons class on this homology group. It also leads to an independent proof of the analytic relationship between volume and Chern-Simons invariant of hyperbolic 3-manifolds conjectured by Neumann a"
  },
  "0311459v2": {
    "title": "A few remarks about symplectic filling",
    "authors": [
      "Yakov Eliashberg"
    ],
    "year": 2003,
    "arxiv_id": "0311459v2",
    "pdf_url": "https://arxiv.org/pdf/math/0311459v2",
    "abstract": "We show that any compact symplectic manifold (W,\u03c9) with boundary embeds as a domain into a closed symplectic manifold, provided that there exists a contact plane \u03beon dW which is weakly compatible with omega, i.e. the restriction \u03c9|\u03bedoes not vanish and the contact orientation of dW and its orientation as the boundary of the symplectic manifold W coincide. This result provides a useful tool for new "
  },
  "0311458v2": {
    "title": "The algebra of knotted trivalent graphs and Turaev's shadow world",
    "authors": [
      "Dylan P. Thurston"
    ],
    "year": 2003,
    "arxiv_id": "0311458v2",
    "pdf_url": "https://arxiv.org/pdf/math/0311458v2",
    "abstract": "Knotted trivalent graphs (KTGs) form a rich algebra with a few simple operations: connected sum, unzip, and bubbling. With these operations, KTGs are generated by the unknotted tetrahedron and Moebius strips. Many previously known representations of knots, including knot diagrams and non-associative tangles, can be turned into KTG presentations in a natural way.\n  Often two sequences of KTG operat"
  },
  "0105028v4": {
    "title": "A rational noncommutative invariant of boundary links",
    "authors": [
      "Stavros Garoufalidis",
      "Andrew Kricker"
    ],
    "year": 2001,
    "arxiv_id": "0105028v4",
    "pdf_url": "https://arxiv.org/pdf/math/0105028v4",
    "abstract": "In 1999, Rozansky conjectured the existence of a rational presentation of the Kontsevich integral of a knot. Roughly speaking, this rational presentation of the Kontsevich integral would sum formal power series into rational functions with prescribed denominators. Rozansky's conjecture was soon proven by the second author. We begin our paper by reviewing Rozansky's conjecture and the main ideas th"
  },
  "1609.04846v1": {
    "title": "A Tutorial about Random Neural Networks in Supervised Learning",
    "authors": [
      "Sebasti\u00e1n Basterrech",
      "Gerardo Rubino"
    ],
    "year": 2016,
    "arxiv_id": "1609.04846v1",
    "pdf_url": "https://arxiv.org/pdf/1609.04846v1",
    "abstract": "Random Neural Networks (RNNs) are a class of Neural Networks (NNs) that can also be seen as a specific type of queuing network. They have been successfully used in several domains during the last 25 years, as queuing networks to analyze the performance of resource sharing in many engineering areas, as learning tools and in combinatorial optimization, where they are seen as neural systems, and also"
  },
  "0505221v1": {
    "title": "Sasakian Geometry and Einstein Metrics on Spheres",
    "authors": [
      "Charles P. Boyer",
      "Krzysztof Galicki"
    ],
    "year": 2005,
    "arxiv_id": "0505221v1",
    "pdf_url": "https://arxiv.org/pdf/math/0505221v1",
    "abstract": "This paper is based on a talk presented by the first author at the Short Program on Riemannian Geometry that took place at the Centre de Recherche Math\u00e9matiques, Universit\u00e9 de Montr\u00e9al, during the period June 28-July 16, 2004. It is a report on our joint work with J\u00e1nos Koll\u00e1r concerning the existence of an abundance of Einstein metrics on odd dimensional spheres, including exotic spheres."
  },
  "2301.11375v4": {
    "title": "How does training shape the Riemannian geometry of neural network representations?",
    "authors": [
      "Jacob A. Zavatone-Veth",
      "Sheng Yang",
      "Julian A. Rubinfien",
      "Cengiz Pehlevan"
    ],
    "year": 2023,
    "arxiv_id": "2301.11375v4",
    "pdf_url": "https://arxiv.org/pdf/2301.11375v4",
    "abstract": "In machine learning, there is a long history of trying to build neural networks that can learn from fewer example data by baking in strong geometric priors. However, it is not always clear a priori what geometric constraints are appropriate for a given task. Here, we explore the possibility that one can uncover useful geometric inductive biases by studying how training molds the Riemannian geometr"
  },
  "2502.01654v1": {
    "title": "Predicting concentration levels of air pollutants by transfer learning and recurrent neural network",
    "authors": [
      "Iat Hang Fong",
      "Tengyue Li",
      "Simon Fong",
      "Raymond K. Wong",
      "Antonio J. Tall\u00f3n-Ballesteros"
    ],
    "year": 2025,
    "arxiv_id": "2502.01654v1",
    "pdf_url": "https://arxiv.org/pdf/2502.01654v1",
    "abstract": "Air pollution (AP) poses a great threat to human health, and people are paying more attention than ever to its prediction. Accurate prediction of AP helps people to plan for their outdoor activities and aids protecting human health. In this paper, long-short term memory (LSTM) recurrent neural networks (RNNs) have been used to predict the future concentration of air pollutants (APS) in Macau. Addi"
  },
  "0708.4298v3": {
    "title": "Dilatation structures in sub-riemannian geometry",
    "authors": [
      "Marius Buliga"
    ],
    "year": 2007,
    "arxiv_id": "0708.4298v3",
    "pdf_url": "https://arxiv.org/pdf/0708.4298v3",
    "abstract": "Based on the notion of dilatation structure arXiv:math/0608536, we give an intrinsic treatment to sub-riemannian geometry, started in the paper arXiv:0706.3644 .\n  Here we prove that regular sub-riemannian manifolds admit dilatation structures. From the existence of normal frames proved by Bellaiche we deduce the rest of the properties of regular sub-riemannian manifolds by using the formalism of "
  },
  "1712.09913v3": {
    "title": "Visualizing the Loss Landscape of Neural Nets",
    "authors": [
      "Hao Li",
      "Zheng Xu",
      "Gavin Taylor",
      "Christoph Studer",
      "Tom Goldstein"
    ],
    "year": 2017,
    "arxiv_id": "1712.09913v3",
    "pdf_url": "https://arxiv.org/pdf/1712.09913v3",
    "abstract": "Neural network training relies on our ability to find \"good\" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and"
  },
  "1802.08117v2": {
    "title": "Topological spaces of persistence modules and their properties",
    "authors": [
      "Peter Bubenik",
      "Tane Vergili"
    ],
    "year": 2018,
    "arxiv_id": "1802.08117v2",
    "pdf_url": "https://arxiv.org/pdf/1802.08117v2",
    "abstract": "Persistence modules are a central algebraic object arising in topological data analysis. The notion of interleaving provides a natural way to measure distances between persistence modules. We consider various classes of persistence modules, including many of those that have been previously studied, and describe the relationships between them. In the cases where these classes are sets, interleaving"
  },
  "1901.02302v2": {
    "title": "Visualising Basins of Attraction for the Cross-Entropy and the Squared Error Neural Network Loss Functions",
    "authors": [
      "Anna Sergeevna Bosman",
      "Andries Engelbrecht",
      "Mard\u00e9 Helbig"
    ],
    "year": 2019,
    "arxiv_id": "1901.02302v2",
    "pdf_url": "https://arxiv.org/pdf/1901.02302v2",
    "abstract": "Quantification of the stationary points and the associated basins of attraction of neural network loss surfaces is an important step towards a better understanding of neural network loss surfaces at large. This work proposes a novel method to visualise basins of attraction together with the associated stationary points via gradient-based random sampling. The proposed technique is used to perform a"
  },
  "1803.02421v2": {
    "title": "Masked Conditional Neural Networks for Audio Classification",
    "authors": [
      "Fady Medhat",
      "David Chesmore",
      "John Robinson"
    ],
    "year": 2018,
    "arxiv_id": "1803.02421v2",
    "pdf_url": "https://arxiv.org/pdf/1803.02421v2",
    "abstract": "We present the ConditionaL Neural Network (CLNN) and the Masked ConditionaL Neural Network (MCLNN) designed for temporal signal recognition. The CLNN takes into consideration the temporal nature of the sound signal and the MCLNN extends upon the CLNN through a binary mask to preserve the spatial locality of the features and allows an automated exploration of the features combination analogous to h"
  },
  "2001.10696v5": {
    "title": "Spiking Inception Module for Multi-layer Unsupervised Spiking Neural Networks",
    "authors": [
      "Mingyuan Meng",
      "Xingyu Yang",
      "Shanlin Xiao",
      "Zhiyi Yu"
    ],
    "year": 2020,
    "arxiv_id": "2001.10696v5",
    "pdf_url": "https://arxiv.org/pdf/2001.10696v5",
    "abstract": "Spiking Neural Network (SNN), as a brain-inspired approach, is attracting attention due to its potential to produce ultra-high-energy-efficient hardware. Competitive learning based on Spike-Timing-Dependent Plasticity (STDP) is a popular method to train an unsupervised SNN. However, previous unsupervised SNNs trained through this method are limited to a shallow network with only one learnable laye"
  },
  "0911.3482v5": {
    "title": "Complexity of Networks (reprise)",
    "authors": [
      "Russell K. Standish"
    ],
    "year": 2009,
    "arxiv_id": "0911.3482v5",
    "pdf_url": "https://arxiv.org/pdf/0911.3482v5",
    "abstract": "Network or graph structures are ubiquitous in the study of complex systems. Often, we are interested in complexity trends of these system as it evolves under some dynamic. An example might be looking at the complexity of a food web as species enter an ecosystem via migration or speciation, and leave via extinction.\n  In a previous paper, a complexity measure of networks was proposed based on the {"
  },
  "2203.15556v1": {
    "title": "Training Compute-Optimal Large Language Models",
    "authors": [
      "Jordan Hoffmann",
      "Sebastian Borgeaud",
      "Arthur Mensch",
      "Elena Buchatskaya",
      "Trevor Cai"
    ],
    "year": 2022,
    "arxiv_id": "2203.15556v1",
    "pdf_url": "https://arxiv.org/pdf/2203.15556v1",
    "abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion "
  },
  "2509.05668v1": {
    "title": "Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian",
    "authors": [
      "Michael Hoffmann",
      "Jophin John",
      "Stefan Schweter",
      "Gokul Ramakrishnan",
      "Hoi-Fong Mak"
    ],
    "year": 2025,
    "arxiv_id": "2509.05668v1",
    "pdf_url": "https://arxiv.org/pdf/2509.05668v1",
    "abstract": "We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as"
  },
  "2202.09061v4": {
    "title": "VLP: A Survey on Vision-Language Pre-training",
    "authors": [
      "Feilong Chen",
      "Duzhen Zhang",
      "Minglun Han",
      "Xiuyi Chen",
      "Jing Shi"
    ],
    "year": 2022,
    "arxiv_id": "2202.09061v4",
    "pdf_url": "https://arxiv.org/pdf/2202.09061v4",
    "abstract": "In the past few years, the emergence of pre-training models has brought uni-modal fields such as computer vision (CV) and natural language processing (NLP) to a new era. Substantial works have shown they are beneficial for downstream uni-modal tasks and avoid training a new model from scratch. So can such pre-trained models be applied to multi-modal tasks? Researchers have explored this problem an"
  },
  "2503.21676v2": {
    "title": "How do language models learn facts? Dynamics, curricula and hallucinations",
    "authors": [
      "Nicolas Zucchet",
      "J\u00f6rg Bornschein",
      "Stephanie Chan",
      "Andrew Lampinen",
      "Razvan Pascanu"
    ],
    "year": 2025,
    "arxiv_id": "2503.21676v2",
    "pdf_url": "https://arxiv.org/pdf/2503.21676v2",
    "abstract": "Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechani"
  },
  "2403.13369v2": {
    "title": "Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting",
    "authors": [
      "Phillip Richter-Pechanski",
      "Philipp Wiesenbach",
      "Dominic M. Schwab",
      "Christina Kiriakou",
      "Nicolas Geis"
    ],
    "year": 2024,
    "arxiv_id": "2403.13369v2",
    "pdf_url": "https://arxiv.org/pdf/2403.13369v2",
    "abstract": "Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent advances in domain-adaptation and prompting methods showed promising results with minimal training data using lightweight masked language models, whi"
  },
  "2412.07942v1": {
    "title": "Neural Scaling Laws Rooted in the Data Distribution",
    "authors": [
      "Ari Brill"
    ],
    "year": 2024,
    "arxiv_id": "2412.07942v1",
    "pdf_url": "https://arxiv.org/pdf/2412.07942v1",
    "abstract": "Deep neural networks exhibit empirical neural scaling laws, with error decreasing as a power law with increasing model or data size, across a wide variety of architectures, tasks, and datasets. This universality suggests that scaling laws may result from general properties of natural learning tasks. We develop a mathematical model intended to describe natural datasets using percolation theory. Two"
  },
  "2502.03009v2": {
    "title": "Scaling Laws for Upcycling Mixture-of-Experts Language Models",
    "authors": [
      "Seng Pei Liew",
      "Takuya Kato",
      "Sho Takase"
    ],
    "year": 2025,
    "arxiv_id": "2502.03009v2",
    "pdf_url": "https://arxiv.org/pdf/2502.03009v2",
    "abstract": "Pretraining large language models (LLMs) is resource-intensive, often requiring months of training time even with high-end GPU clusters. There are two approaches of mitigating such computational demands: reusing smaller models to train larger ones (upcycling), and training computationally efficient models like mixture-of-experts (MoE). In this paper, we study the upcycling of LLMs to MoE models, o"
  },
  "2402.04177v3": {
    "title": "Scaling Laws for Downstream Task Performance of Large Language Models",
    "authors": [
      "Berivan Isik",
      "Natalia Ponomareva",
      "Hussein Hazimeh",
      "Dimitris Paparas",
      "Sergei Vassilvitskii"
    ],
    "year": 2024,
    "arxiv_id": "2402.04177v3",
    "pdf_url": "https://arxiv.org/pdf/2402.04177v3",
    "abstract": "Scaling laws provide important insights that can guide the design of large language models (LLMs). Existing work has primarily focused on studying scaling laws for pretraining (upstream) loss. However, in transfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then finetuned on a downstream task, we often also care about the downstream performance. In this work, we "
  },
  "2403.09832v1": {
    "title": "Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks",
    "authors": [
      "Zhifan Sun",
      "Antonio Valerio Miceli-Barone"
    ],
    "year": 2024,
    "arxiv_id": "2403.09832v1",
    "pdf_url": "https://arxiv.org/pdf/2403.09832v1",
    "abstract": "Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples. Their generality, however, opens them up to subversion"
  },
  "2408.11029v2": {
    "title": "Scaling Law with Learning Rate Annealing",
    "authors": [
      "Howe Tissue",
      "Venus Wang",
      "Lu Wang"
    ],
    "year": 2024,
    "arxiv_id": "2408.11029v2",
    "pdf_url": "https://arxiv.org/pdf/2408.11029v2",
    "abstract": "We find that the cross-entropy loss curves of neural language models empirically adhere to a scaling law with learning rate (LR) annealing over training steps: $$L(s) = L_0 + A\\cdot S_1^{-\u03b1} - C\\cdot S_2,$$ where $L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR curve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\u03b1$ are constant parameters. This formulation takes i"
  }
}