# Downloaded Papers

## TDA Applied to Neural Networks

1. **[TDA of Neural Network Layer Representations](2208.06438_tda_nn_layer_representations.pdf)**
   - Authors: Shahidullah (2022)
   - arXiv: 2208.06438
   - Why relevant: Directly applies persistent homology to NN layer representations; demonstrates topology changes through layers

2. **[Can Neural Networks Learn Persistent Homology Features?](2011.14688_nn_learn_persistent_homology.pdf)**
   - Authors: Mont√∫far, Otter & Wang (2020)
   - arXiv: 2011.14688
   - Why relevant: Shows NNs can approximate topological feature computation

3. **[TDA and Topological Deep Learning Beyond Persistent Homology - A Review](2507.19504_tda_beyond_persistent_homology_review.pdf)**
   - Authors: Various (2025)
   - arXiv: 2507.19504
   - Why relevant: Comprehensive review of TDA methods and tools

4. **[On the Expressivity of Persistent Homology in Graph Learning](2302.09826_expressivity_persistent_homology.pdf)**
   - Authors: Various (2023)
   - arXiv: 2302.09826
   - Why relevant: Theoretical analysis of what PH can capture

5. **[Regularization of Persistent Homology Gradient Computation](2011.05804_persistent_homology_gradient.pdf)**
   - Authors: Various (2020)
   - arXiv: 2011.05804
   - Why relevant: Methods for making PH differentiable (potential for optimization)

6. **[Intrinsic Persistent Homology via Density-Based Metric Learning](2012.07621_intrinsic_persistent_homology.pdf)**
   - Authors: Various (2020)
   - arXiv: 2012.07621
   - Why relevant: PH estimation under the manifold assumption in high dimensions

## Geometry and Intrinsic Dimension of Representations

7. **[How Does Training Shape the Riemannian Geometry of NN Representations?](2309.00254_riemannian_geometry_nn_representations.pdf)**
   - Authors: Various (2023)
   - arXiv: 2309.00254
   - Why relevant: Riemannian geometry perspective on how training affects representation geometry

8. **[Intrinsic Dimensions of Language Fractal Structures](2311.10217_intrinsic_dimensions_language_fractal.pdf)**
   - Authors: Various (2023)
   - arXiv: 2311.10217
   - Why relevant: Novel characterization of language structure via intrinsic dimension

9. **[Intrinsic Dimensionality Explains the Effectiveness of LM Fine-Tuning](2012.13255_intrinsic_dimensionality_finetuning.pdf)**
   - Authors: Aghajanyan, Zettlemoyer & Gupta (2020)
   - arXiv: 2012.13255
   - Why relevant: Foundational paper connecting intrinsic dimension to LM generalization

10. **[Tracing the Representation Geometry of LMs from Pretraining to Post-training](2509.23024_representation_geometry_lm.pdf)**
    - Authors: Li et al. (2025)
    - arXiv: 2509.23024
    - Why relevant: Discovers three geometric phases during LM pretraining using spectral analysis

11. **[The Linear Representation Hypothesis and the Geometry of LLMs](2311.03658_linear_representation_hypothesis.pdf)**
    - Authors: Various (2023)
    - arXiv: 2311.03658
    - Why relevant: Geometric understanding of concept representation in LLMs

12. **[The Geometry of Truth: Emergent Linear Structure in LLM Representations](2310.06824_geometry_truth_llm.pdf)**
    - Authors: Various (2023)
    - arXiv: 2310.06824
    - Why relevant: Shows semantic properties are geometrically encoded

13. **[Characterizing Truthfulness in LLMs with Local Intrinsic Dimension](2402.18048_truthfulness_intrinsic_dimension.pdf)**
    - Authors: Various (2024)
    - arXiv: 2402.18048
    - Why relevant: Uses LID to characterize LLM behavior

14. **[Memorization in LMs through the Lens of Intrinsic Dimension](2506.09591_memorization_intrinsic_dimension.pdf)**
    - Authors: Various (2025)
    - arXiv: 2506.09591
    - Why relevant: Intrinsic dimension applied to memorization analysis

15. **[Less is More: Local Intrinsic Dimensions of Contextual LMs](2506.01034_local_intrinsic_dimensions_lm.pdf)**
    - Authors: Various (2025)
    - arXiv: 2506.01034
    - Why relevant: Local intrinsic dimension analysis of LLMs

16. **[A Comparative Study of Learning Paradigms in LLMs via Intrinsic Dimension](2412.06245_learning_paradigms_intrinsic_dimension.pdf)**
    - Authors: Various (2024)
    - arXiv: 2412.06245
    - Why relevant: Compares SFT vs ICL through intrinsic dimension lens

17. **[Vocabulary Embeddings Organize Linguistic Structure Early in Training](2505.00773_vocab_embeddings_linguistic_structure.pdf)**
    - Authors: Various (2025)
    - arXiv: 2505.00773
    - Why relevant: Shows early geometric organization in LM training

## Scaling Laws and Resource-Constrained Training

18. **[Training Compute-Optimal LLMs (Chinchilla)](2203.15556_chinchilla_compute_optimal.pdf)**
    - Authors: Hoffmann et al. (2022)
    - arXiv: 2203.15556
    - Why relevant: Foundational paper defining compute-optimal training

19. **[Beyond Chinchilla-Optimal: Accounting for Inference](2401.00448_beyond_chinchilla_optimal.pdf)**
    - Authors: Various (2023)
    - arXiv: 2401.00448
    - Why relevant: Extends Chinchilla to consider inference costs

20. **[Evaluating the Robustness of Chinchilla Compute-Optimal Scaling](2509.23963_robustness_chinchilla_scaling.pdf)**
    - Authors: Various (2025)
    - arXiv: 2509.23963
    - Why relevant: Tests robustness of scaling laws

21. **[Reconciling Kaplan and Chinchilla Scaling Laws](2406.12907_reconciling_scaling_laws.pdf)**
    - Authors: Various (2024)
    - arXiv: 2406.12907
    - Why relevant: Resolves discrepancies between major scaling formulations

22. **[Scaling Law with Learning Rate Annealing](2408.11029_scaling_law_lr_annealing.pdf)**
    - Authors: Various (2024)
    - arXiv: 2408.11029
    - Why relevant: Scaling laws that account for training dynamics

23. **[Scaling Laws for Downstream Task Performance](2402.04177_scaling_laws_downstream.pdf)**
    - Authors: Various (2024)
    - arXiv: 2402.04177
    - Why relevant: Connects scaling to downstream performance

24. **[Neural Scaling Laws Rooted in the Data Distribution](2404.10102_scaling_laws_data_distribution.pdf)**
    - Authors: Various (2024)
    - arXiv: 2404.10102
    - Why relevant: Data-centric perspective on scaling laws

## Supporting

25. **[Visualizing the Loss Landscape of Neural Nets](1712.09913_loss_landscape_visualization.pdf)**
    - Authors: Li et al. (2017)
    - arXiv: 1712.09913
    - Why relevant: Complementary geometric perspective on training dynamics
